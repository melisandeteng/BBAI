{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20pt; font-weight: bold; margin: 1em 0em 1em 0em\">Climate Change Hackathon: Climate data preprocessing</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/climate-daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x', 'y', 'STATION_NAME', 'STN_ID', 'CLIMATE_IDENTIFIER', 'ID',\n",
       "       'LOCAL_DATE', 'PROVINCE_CODE', 'LOCAL_YEAR', 'LOCAL_MONTH', 'LOCAL_DAY',\n",
       "       'MEAN_TEMPERATURE', 'MEAN_TEMPERATURE_FLAG', 'MIN_TEMPERATURE',\n",
       "       'MIN_TEMPERATURE_FLAG', 'MAX_TEMPERATURE', 'MAX_TEMPERATURE_FLAG',\n",
       "       'TOTAL_PRECIPITATION', 'TOTAL_PRECIPITATION_FLAG', 'TOTAL_RAIN',\n",
       "       'TOTAL_RAIN_FLAG', 'TOTAL_SNOW', 'TOTAL_SNOW_FLAG', 'SNOW_ON_GROUND',\n",
       "       'SNOW_ON_GROUND_FLAG', 'DIRECTION_MAX_GUST', 'DIRECTION_MAX_GUST_FLAG',\n",
       "       'SPEED_MAX_GUST', 'SPEED_MAX_GUST_FLAG', 'COOLING_DEGREE_DAYS',\n",
       "       'COOLING_DEGREE_DAYS_FLAG', 'HEATING_DEGREE_DAYS',\n",
       "       'HEATING_DEGREE_DAYS_FLAG', 'MIN_REL_HUMIDITY', 'MIN_REL_HUMIDITY_FLAG',\n",
       "       'MAX_REL_HUMIDITY', 'MAX_REL_HUMIDITY_FLAG'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the data we want to use\n",
    "df0 = df[['STATION_NAME','LOCAL_YEAR', 'LOCAL_MONTH', 'LOCAL_DAY',\n",
    "    'MEAN_TEMPERATURE', 'MIN_TEMPERATURE', 'MAX_TEMPERATURE',\n",
    "    'TOTAL_PRECIPITATION', 'TOTAL_RAIN', 'TOTAL_SNOW', 'SNOW_ON_GROUND',\n",
    "    'DIRECTION_MAX_GUST','SPEED_MAX_GUST', 'COOLING_DEGREE_DAYS',\n",
    "    'HEATING_DEGREE_DAYS','MIN_REL_HUMIDITY','MAX_REL_HUMIDITY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MONTREAL/PIERRE ELLIOTT TRUDEAU INTL',\n",
       " 'MONTREAL/ST-HUBERT',\n",
       " 'MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract each station's data\n",
    "station_names = df0['STATION_NAME'].unique().tolist()\n",
    "station_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 3 separate DataFrame: one per station in Montreal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df0.loc[df0.STATION_NAME=='MONTREAL/PIERRE ELLIOTT TRUDEAU INTL']\n",
    "df1 = df1[df1.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df0.loc[df0.STATION_NAME=='MONTREAL/ST-HUBERT']\n",
    "df2 = df2[df2.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df0.loc[df0.STATION_NAME=='MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A']\n",
    "df3 = df3[df3.columns[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate the data from days to weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_per_week(df):\n",
    "    \n",
    "    # aggregate the lines per weeks\n",
    "    df_per_week = []\n",
    "    for year in df['LOCAL_YEAR'].unique():\n",
    "        df_y = df.loc[df.LOCAL_YEAR==year]\n",
    "        cpt=0\n",
    "        for month in df_y['LOCAL_MONTH'].unique():\n",
    "            df_y_m = df_y.loc[df_y.LOCAL_MONTH==month]\n",
    "            df_y_m_s = []\n",
    "            n_week_memory = None\n",
    "            for index, row in df_y_m.iterrows():\n",
    "                line = row.values.tolist()\n",
    "                day = line[2]\n",
    "                n_year, n_week = date(int(year), int(month), int(day)).isocalendar()[:2]\n",
    "                id_week = int(n_year)*100+int(n_week)\n",
    "                df_y_m_s.append([id_week] + line[3:])\n",
    "                if n_week_memory is None:\n",
    "                    n_week_memory = n_week\n",
    "                elif n_week_memory<n_week:\n",
    "                    df_per_week.append(df_y_m_s)\n",
    "                    df_y_m_s = []\n",
    "    \n",
    "    # make the summary of each week\n",
    "    # the columns are: 'MEAN_TEMPERATURE', 'MIN_TEMPERATURE', 'MAX_TEMPERATURE',\n",
    "    #                  'TOTAL_PRECIPITATION', 'TOTAL_RAIN', 'TOTAL_SNOW', 'SNOW_ON_GROUND',\n",
    "    #                  'DIRECTION_MAX_GUST','SPEED_MAX_GUST', 'COOLING_DEGREE_DAYS',\n",
    "    #                  'HEATING_DEGREE_DAYS'\n",
    "    \n",
    "    df_per_week_summary = []\n",
    "    \n",
    "    for data_week in df_per_week:\n",
    "        data_week = np.array(data_week)\n",
    "        summary = [int(data_week[0,0])] # Id week\n",
    "        # summary of the week\n",
    "        summary.append(np.nanmean(data_week[:,1])) # Mean temp\n",
    "        summary.append(np.nanmin(data_week[:,2])) # min temp\n",
    "        summary.append(np.nanmax(data_week[:,3])) # max temp\n",
    "        summary.append(np.nansum(data_week[:,4])) # total precipitation\n",
    "        summary.append(np.nansum(data_week[:,5])) # total rain\n",
    "        summary.append(np.nansum(data_week[:,6])) # total snow\n",
    "        summary.append(np.nanmean(data_week[:,8])) # direction max gust\n",
    "        summary.append(np.nanmax(data_week[:,9])) # speed max gust\n",
    "        summary.append(np.nanmean(data_week[:,10])) # cooling degree days\n",
    "        summary.append(np.nanmean(data_week[:,11])) # heating degree days\n",
    "        summary.append(len(np.where(data_week[:,3]>20)[0])) # nb day per week where temp >20\n",
    "        summary.append(len(np.where(data_week[:,3]>25)[0])) # nb day per week where temp >25\n",
    "        summary.append(len(np.where(data_week[:,3]>30)[0])) # nb day per week where temp >30\n",
    "        summary.append(len(np.where(data_week[:,2]<10)[0])) # nb day per week where temp <10\n",
    "        summary.append(len(np.where(data_week[:,2]<0)[0])) # nb day per week where temp <0\n",
    "        summary.append(len(np.where(data_week[:,2]<-5)[0])) # nb day per week where temp <-5\n",
    "        summary.append(len(np.where(data_week[:,2]<-10)[0])) # nb day per week where temp <-10\n",
    "        summary.append(len(np.where(data_week[:,4]>5)[0])) # nb day day per week where precipitation > 5\n",
    "    \n",
    "        df_per_week_summary.append(summary)\n",
    "    \n",
    "    df_week = pd.DataFrame(df_per_week_summary, columns=['id_week',\n",
    "    'MEAN_TEMPERATURE', 'MIN_TEMPERATURE', 'MAX_TEMPERATURE',\n",
    "    'TOTAL_PRECIPITATION', 'TOTAL_RAIN', 'TOTAL_SNOW',\n",
    "    'DIRECTION_MAX_GUST','SPEED_MAX_GUST', 'COOLING_DEGREE_DAYS',\n",
    "    'HEATING_DEGREE_DAYS',\n",
    "    'nb_j_t_sup20_1w', 'nb_j_t_sup25_1w', 'nb_j_t_sup30_1w', 'nb_j_t_inf_10_1w', 'nb_j_t_inf_0_1w', 'nb_jt__inf_m5_1w',\n",
    "    'nb_j_t_inf_m10_1w', 'nb_j_precip_sup_5_1w'])\n",
    "    \n",
    "    return(df_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: Mean of empty slice\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: RuntimeWarning: All-NaN slice encountered\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: All-NaN slice encountered\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: Mean of empty slice\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: Mean of empty slice\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in greater\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in greater\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in greater\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in less\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in less\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in less\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in less\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: Mean of empty slice\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: All-NaN slice encountered\n",
      "L:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "df1_w = aggregate_data_per_week(df1)\n",
    "df2_w = aggregate_data_per_week(df2)\n",
    "df3_w = aggregate_data_per_week(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate the data from the 3 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_week = []\n",
    "\n",
    "for week in df1_w.id_week.unique()[:]:\n",
    "    \n",
    "    df1_week = df1_w.loc[df1_w.id_week==week].values\n",
    "    \n",
    "    # if there is data for the 3 station\n",
    "    if week in df2_w.id_week.unique() and week in df3_w.id_week.unique():\n",
    "        df2_week = df2_w.loc[df2_w.id_week==week].values\n",
    "        df3_week = df3_w.loc[df3_w.id_week==week].values\n",
    "        # concatenate the 3 lines, take the mean\n",
    "        data_week.append(np.nanmean(np.concatenate((df1_week, df2_week, df3_week), axis=0), axis=0))\n",
    "        \n",
    "    # if there is data for only station 1 and 2\n",
    "    elif week in df2_w.id_week.unique():\n",
    "        df2_week = df2_w.loc[df2_w.id_week==week].values\n",
    "        # take the mean of the 2 values\n",
    "        data_week.append(np.nanmean(np.concatenate((df1_week, df2_week), axis=0), axis=0))\n",
    "    \n",
    "    # if there is data for only station 1 and 3\n",
    "    elif week in df3_w.id_week.unique():\n",
    "        df3_week = df3_w.loc[df3_w.id_week==week].values\n",
    "        # take the mean of the 2 values\n",
    "        data_week.append(np.nanmean(np.concatenate((df1_week, df3_week), axis=0), axis=0))\n",
    "    \n",
    "    # else there is only data for station 1\n",
    "    else:\n",
    "        data_week.append(df1_week[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment the data with rolling averages/min/max etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "# initialize the memories\n",
    "week_m1 = np.reshape(data_week[0], (1, len(data_week[0])))\n",
    "week_m2 = np.reshape(data_week[0], (1, len(data_week[0])))\n",
    "week_m3 = np.reshape(data_week[0], (1, len(data_week[0])))\n",
    "\n",
    "for week in data_week:\n",
    "    \n",
    "    liste_week = week.tolist()\n",
    "    \n",
    "    # data from last weeks\n",
    "    week0 = np.reshape(week, (1,len(week)))\n",
    "    weeks_4 = np.concatenate((week_m3, week_m2, week_m1, week0), axis=0)\n",
    "    weeks_3 = np.concatenate((week_m2, week_m1, week0), axis=0)\n",
    "    weeks_2 = np.concatenate((week_m1, week0), axis=0)\n",
    "    \n",
    "    for weeks in [weeks_4, weeks_3, weeks_2]:\n",
    "    \n",
    "        liste_week += [np.nanmean(weeks[:,1], axis=0)] # mean temp\n",
    "        liste_week += [np.nanmin(weeks[:,2], axis=0)] # min temp\n",
    "        liste_week += [np.nanmax(weeks[:,3], axis=0)] # max temp\n",
    "        liste_week += [np.nansum(weeks[:,4], axis=0)] # tot precipitation\n",
    "        liste_week += [np.nansum(weeks[:,5], axis=0)] # tot rain\n",
    "        liste_week += [np.nansum(weeks[:,6], axis=0)] # tot snow\n",
    "        liste_week += [np.nansum(weeks[:,11], axis=0)] # nb days per x weeks temp>20\n",
    "        liste_week += [np.nansum(weeks[:,12], axis=0)] # nb days per x weeks temp>25\n",
    "        liste_week += [np.nansum(weeks[:,13], axis=0)] # nb days per x weeks temp>30\n",
    "        liste_week += [np.nansum(weeks[:,14], axis=0)] # nb days per x weeks temp<10\n",
    "        liste_week += [np.nansum(weeks[:,15], axis=0)] # nb days per x weeks temp<0\n",
    "        liste_week += [np.nansum(weeks[:,16], axis=0)] # nb days per x weeks temp<-5\n",
    "        liste_week += [np.nansum(weeks[:,17], axis=0)] # nb days per x weeks temp<-10\n",
    "        liste_week += [np.nansum(weeks[:,18], axis=0)] # nb days per x weeks precipitation > 5\n",
    "    \n",
    "    week_m3 = week_m2\n",
    "    week_m2 = week_m1\n",
    "    week_m1 = week0\n",
    "    \n",
    "    all_data.append(liste_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Columns = ['id_week', 'MEAN_TEMPERATURE', 'MIN_TEMPERATURE', 'MAX_TEMPERATURE', 'TOTAL_PRECIPITATION',\n",
    "           'TOTAL_RAIN', 'TOTAL_SNOW', 'DIRECTION_MAX_GUST','SPEED_MAX_GUST', 'COOLING_DEGREE_DAYS',\n",
    "           'HEATING_DEGREE_DAYS', 'nb_j_t_sup20_1w', 'nb_j_t_sup25_1w', 'nb_j_t_sup30_1w', 'nb_j_t_inf_10_1w',\n",
    "           'nb_j_t_inf_0_1w', 'nb_jt__inf_m5_1w', 'nb_j_t_inf_m10_1w', 'nb_j_precip_sup_5_1w',\n",
    "          'mean_t_4w', 'min_t_4w', 'max_t_4w', 'tot_precip_4w', 'tot_rain_4w', 'tot_snow_4w',\n",
    "          'nb_j_t_sup20_4w', 'nb_j_t_sup25_4w', 'nb_j_t_sup30_4w', 'nb_j_t_inf_10_4w',\n",
    "           'nb_j_t_inf_0_4w', 'nb_jt__inf_m5_4w', 'nb_j_t_inf_m10_4w', 'nb_j_precip_sup_5_4w',\n",
    "          'mean_t_3w', 'min_t_3w', 'max_t_3w', 'tot_precip_3w', 'tot_rain_3w', 'tot_snow_3w',\n",
    "          'nb_j_t_sup20_3w', 'nb_j_t_sup25_3w', 'nb_j_t_sup30_3w', 'nb_j_t_inf_10_3w',\n",
    "           'nb_j_t_inf_0_3w', 'nb_jt__inf_m5_3w', 'nb_j_t_inf_m10_3w', 'nb_j_precip_sup_5_3w',\n",
    "          'mean_t_2w', 'min_t_4w', 'max_t_2w', 'tot_precip_2w', 'tot_rain_2w', 'tot_snow_2w',\n",
    "          'nb_j_t_sup20_2w', 'nb_j_t_sup25_2w', 'nb_j_t_sup30_2w', 'nb_j_t_inf_10_2w',\n",
    "           'nb_j_t_inf_0_2w', 'nb_jt__inf_m5_2w', 'nb_j_t_inf_m10_2w', 'nb_j_precip_sup_5_2w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(all_data[4:], columns=Columns)\n",
    "df_final.fillna(method='ffill', inplace=True)\n",
    "df_final.to_csv('climate_per_week_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
